{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdorodch/ITSC3155-Demos/blob/master/Hour2_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "oU8zRXv8QHlm"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mh9jBR_cQ5_a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0afe4324-6f89-4736-e04e-d4ec493a6686"
      },
      "cell_type": "code",
      "source": [
        "#env = gym.make(\"FrozenLake-v1\")\n",
        "desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "env= gym.make('FrozenLake-v1', desc=desc, map_name=\"4x4\", is_slippery=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Frozen Lake environment\n",
        "https://www.gymlibrary.dev/environments/toy_text/frozen_lake/\n",
        "\n",
        "Reward schedule:\n",
        "\n",
        "Reach goal(G): +1     \n",
        "\n",
        "Reach hole(H): 0\n",
        "\n",
        "Reach frozen(F): 0"
      ],
      "metadata": {
        "id": "ZGHJ0AZ8fp2I"
      }
    },
    {
      "metadata": {
        "id": "Uc0xDVd_Q-C8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db62ab67-f463-4572-da1a-a42f71cb5eca"
      },
      "cell_type": "code",
      "source": [
        "print(env.action_space)\n",
        "print(env.observation_space)\n",
        "#desc=[\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(4)\n",
            "Discrete(16)\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "17XBPCecAsZG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c1d9fd8-f686-4205-9b58-d1b6994bf137"
      },
      "cell_type": "code",
      "source": [
        "# Create our Q table with state_size rows and action_size columns (16x4)\n",
        "action_size = env.action_space.n\n",
        "state_size = env.observation_space.n\n",
        "print(state_size, action_size)\n",
        "\n",
        "qtable = np.zeros((state_size, action_size))\n",
        "print(qtable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16 4\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q-learning\n",
        "Choose an action in state s, observe the reward, and update Q-value associated with (s,a)"
      ],
      "metadata": {
        "id": "9Twv_ey9Xupe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_episodes = 1000\n",
        "max_steps = 100  #try maximum 100 steps inside a game, to avoid a loop\n",
        "epsilon = 0.5    # Exploration rate\n",
        "learning_rate = 0.7    # Learning rate\n",
        "gamma = 0.95                 # Discounting rate\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.01            # Minimum exploration probability\n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "rHFFJmLysjqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Dqqo_8LA5De",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70d4346-d1df-48f4-e9a4-5ec3b89b40ee"
      },
      "cell_type": "code",
      "source": [
        "def q_learning(episodes,epsilon):\n",
        "  rewards = []\n",
        "  # Each episode happen after a game ends (agent succeed or die)\n",
        "  for episode in range(episodes):\n",
        "      # Reset the environment\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      total_rewards = 0\n",
        "      if episode % 1000 == 0:\n",
        "          print(\"Episode: \", episode)\n",
        "\n",
        "      # The Q-Table learning algorithm\n",
        "      for step in range(max_steps):\n",
        "          # 3. Choose an action a in the current world state (s)\n",
        "          ## First we randomize a number\n",
        "          exploit_rate = random.uniform(0, 1)\n",
        "\n",
        "          ## If this number > 0.2, exploit\n",
        "          if exploit_rate > epsilon:\n",
        "              action = np.argmax(qtable[state,:])\n",
        "              #print(exp_exp_tradeoff, \"action\", action)\n",
        "          # Else doing a random choice --> exploration\n",
        "          else: #explore\n",
        "              action = env.action_space.sample()\n",
        "              #print(\"action random\", action)\n",
        "\n",
        "          # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "          qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n",
        "\n",
        "          total_rewards += reward\n",
        "          state = new_state  #update the state\n",
        "\n",
        "          # If done (if we're dead) : finish episode\n",
        "          if done == True:\n",
        "              break\n",
        "\n",
        "      # Reduce learning rate, to converge\n",
        "      #learning_rate = 1/(episode+1)\n",
        "      epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "      rewards.append(total_rewards)\n",
        "\n",
        "  print (\"Average score over episodes: \" +  str(sum(rewards)/total_episodes))\n",
        "  print(qtable)\n",
        "\n",
        "q_learning(1000,epsilon)\n",
        "#action 0: LEFT, 1: DOWN, 2: RIGHT, 3: UP\n",
        "#state = current_row * nrows + current_col"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:  0\n",
            "Average score over episodes: 0.0\n",
            "[[0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]\n",
            " [0. 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "R5czk9qTBQIU"
      },
      "cell_type": "markdown",
      "source": [
        "## Test performance"
      ]
    },
    {
      "metadata": {
        "id": "Bt8UsREaBNkJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31e63992-16e3-4959-905d-ba93fec1b49c"
      },
      "cell_type": "code",
      "source": [
        "test_episodes = 10\n",
        "def test_agent(test_episodes):\n",
        "  for episode in range(test_episodes):\n",
        "      state = env.reset()\n",
        "      step = 0\n",
        "      done = False\n",
        "      print(\"****************************************************\")\n",
        "      print(\"EPISODE \", episode)\n",
        "\n",
        "      for step in range(max_steps):\n",
        "          # Take the action (index) that have the maximum expected future reward given that state\n",
        "          action = np.argmax(qtable[state,:])\n",
        "          new_state, reward, done, info = env.step(action)\n",
        "          if done:\n",
        "              # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n",
        "              #env.render()\n",
        "              if new_state == 15:\n",
        "                  print(\"We reached our Goal üèÜ\")\n",
        "              else:\n",
        "                  print(\"We fell into a hole ‚ò†Ô∏è\")\n",
        "\n",
        "              # We print the number of step it took.\n",
        "              print(\"Number of steps\", step)\n",
        "\n",
        "              break\n",
        "          state = new_state\n",
        "  env.close()\n",
        "\n",
        "test_agent(test_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "We fell into a hole ‚ò†Ô∏è\n",
            "Number of steps 99\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning(10000,epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ONnpeZuQlE",
        "outputId": "28227e4b-8939-4243-ad4e-ab4e90c5cbf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:  0\n",
            "Episode:  1000\n",
            "Episode:  2000\n",
            "Episode:  3000\n",
            "Episode:  4000\n",
            "Episode:  5000\n",
            "Episode:  6000\n",
            "Episode:  7000\n",
            "Episode:  8000\n",
            "Episode:  9000\n",
            "Average score over episodes: 9.688\n",
            "[[0.73509189 0.6983373  0.77378094 0.73509189]\n",
            " [0.73509189 0.         0.81450625 0.77378094]\n",
            " [0.77378094 0.857375   0.77378094 0.81450625]\n",
            " [0.81450625 0.         0.67956691 0.6267943 ]\n",
            " [0.69829417 0.65757668 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.         0.81450625]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.45127704 0.         0.56288833 0.6983373 ]\n",
            " [0.66342043 0.         0.63175    0.        ]\n",
            " [0.63024941 0.95       0.         0.857375  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.86542513 0.95       0.44117112]\n",
            " [0.9025     0.95       1.         0.9025    ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_agent(test_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX-jlo1tuYq8",
        "outputId": "b93a933a-ecb9-47fb-acb7-e71585b6087f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_learning(30000,epsilon)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu9h9GXCuuU8",
        "outputId": "7a60632e-358f-46ea-9e0b-ba5d8ac5db3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode:  0\n",
            "Episode:  1000\n",
            "Episode:  2000\n",
            "Episode:  3000\n",
            "Episode:  4000\n",
            "Episode:  5000\n",
            "Episode:  6000\n",
            "Episode:  7000\n",
            "Episode:  8000\n",
            "Episode:  9000\n",
            "Episode:  10000\n",
            "Episode:  11000\n",
            "Episode:  12000\n",
            "Episode:  13000\n",
            "Episode:  14000\n",
            "Episode:  15000\n",
            "Episode:  16000\n",
            "Episode:  17000\n",
            "Episode:  18000\n",
            "Episode:  19000\n",
            "Episode:  20000\n",
            "Episode:  21000\n",
            "Episode:  22000\n",
            "Episode:  23000\n",
            "Episode:  24000\n",
            "Episode:  25000\n",
            "Episode:  26000\n",
            "Episode:  27000\n",
            "Episode:  28000\n",
            "Episode:  29000\n",
            "Average score over episodes: 29.467\n",
            "[[0.73509189 0.77378094 0.77378094 0.73509189]\n",
            " [0.73509189 0.         0.81450625 0.77378094]\n",
            " [0.77378094 0.857375   0.77378094 0.81450625]\n",
            " [0.81450625 0.         0.77377476 0.77378093]\n",
            " [0.77378094 0.81450625 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.         0.81450625]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.81450625 0.         0.857375   0.77378094]\n",
            " [0.81450625 0.9025     0.9025     0.        ]\n",
            " [0.857375   0.95       0.         0.857375  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.95       0.857375  ]\n",
            " [0.9025     0.95       1.         0.9025    ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_agent(test_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jvd4u3equzzm",
        "outputId": "1d90b5cf-40be-411d-a8f4-66f34c42b9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "****************************************************\n",
            "EPISODE  0\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  1\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  2\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  3\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  4\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  5\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  6\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  7\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  8\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n",
            "****************************************************\n",
            "EPISODE  9\n",
            "We reached our Goal üèÜ\n",
            "Number of steps 5\n"
          ]
        }
      ]
    }
  ]
}