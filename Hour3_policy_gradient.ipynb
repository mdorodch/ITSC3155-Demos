{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdorodch/ITSC3155-Demos/blob/master/Hour3_policy_gradient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cd /content/gdrive/MyDrive/workshop_RLHF"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICIyx6IvTt6n",
        "outputId": "d247f92b-b22a-4b79-8ab7-f53181e2c348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/workshop_RLHF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical #Creates a categorical distribution parameterized by input"
      ],
      "metadata": {
        "id": "dXy0u1cf4Q5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))\n",
        "m.sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLnnszLx5aqX",
        "outputId": "db157141-fd85-4767-d0f1-f43ba8ba44b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_TaLKFXo_8"
      },
      "source": [
        "### The CartPole-v1 environment\n",
        "\n",
        "The episode ends if: The pole Angle is greater than ±12° ; Cart Position is greater than ±2.4; Episode length is greater than 500\n",
        "\n",
        "We get a reward +1 every timestep the Pole stays in the equilibrium."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "eval_env = gym.make(\"CartPole-v1\") # Create the evaluation env"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZyiGVNp2dtZ",
        "outputId": "b93957fc-286b-4d07-b7a9-15a67239756c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POOOk15_K6KA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8cfe32-9886-451b-8d49-bb6c7476b98b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
            "Discrete(2)\n",
            "4 2\n"
          ]
        }
      ],
      "source": [
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "# Get the state space and action space\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "print(state_size, action_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMLFrjiBNLYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fe74e0b-2302-4307-ef71-56940f574983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample state [ 2.4505110e+00 -1.3210601e+38 -2.2125986e-01  4.2570131e+37]\n",
            "Action Sample 1\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample state\", env.observation_space.sample()) # Get a random state\n",
        "print(\"Action Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaJu5FeZxXGY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57dca395-294b-4f60-92af-5abe44a23219"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=[1, 2,4]\n",
        "a.unsqueeze(0)\n",
        "[[1][2][4]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "-uZ1imbWUd70",
        "outputId": "160579cc-b76f-4243-c5bb-46dd2ac0af84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-1f2bfc0e203e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unsqueeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_UHf49N9i4"
      },
      "outputs": [],
      "source": [
        "#build a neural network that is a policy function\n",
        "#Two fully connected layers (fc1 and fc2).\n",
        "#Using ReLU as activation function of fc1\n",
        "#Using Softmax to output a probability distribution over actions\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()  #sample a random action from current distribution\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can't use `action = np.argmax(m)` since it will always output the action that have the highest probability. We need to replace with `action = m.sample()` that will sample an action from the probability distribution P(.|s)"
      ],
      "metadata": {
        "id": "c-20i7Pk0l1T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3lWyVXBVfl6"
      },
      "outputs": [],
      "source": [
        "# Create policy and place it to the device\n",
        "h_size= 16\n",
        "lr=1e-2\n",
        "myPolicy = Policy(state_size, action_size, h_size).to(device)\n",
        "optimizer = optim.Adam(myPolicy.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWhQyJjfpEt"
      },
      "source": [
        "## Use REINFORCE training method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utRe1NgtVBYF"
      },
      "outputs": [],
      "source": [
        "#Hyper parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCNvyElRStWG"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer):\n",
        "    episodes= 400\n",
        "    max_t= 1000\n",
        "    gamma= 1.0 #when this is 1, there is no discount\n",
        "\n",
        "    # Help us to calculate the score during the training\n",
        "    scores_deque = deque(maxlen=100) #double-ended queue\n",
        "    scores = []\n",
        "\n",
        "    for episode in range(episodes):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "\n",
        "        #take multiple period's actions and gather reward\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        ##  \"returns\" will hold the returns in chronological order, from t=0 to t=n_steps\n",
        "        ## thanks to the appendleft() function which allows to append to the position 0 in constant time O(1)\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards) #rewards is an array that holds trials\n",
        "        for t in range(n_steps)[::-1]: #reversed list, [t-1, t-2, ..., 0], the last item is 0\n",
        "            #E.g. print(range(5)[::-1])  will generate range(4, -1, -1)\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft(gamma*disc_return_t + rewards[t]   )\n",
        "\n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()  ## eps is the smallest representable float, added to avoid numerical instabilities\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # gradient descent\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()  #update the weight\n",
        "\n",
        "        if episode % 100 == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGf-hQCnfouB"
      },
      "outputs": [],
      "source": [
        "scores = reinforce(myPolicy,optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qajj2kXqhB3g"
      },
      "source": [
        "## Define evaluation method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FamHmxyhBEU"
      },
      "outputs": [],
      "source": [
        "eval_episodes= 10\n",
        "def evaluate_agent(env, max_steps, eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The Reinforce agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohGSXDyHh0xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89ede1a2-e4a7-4e39-f576-faaba28a3b7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "evaluate_agent(eval_env,max_t,eval_episodes,myPolicy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "\n",
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  images = []\n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # Take the action (index) that have the maximum expected future reward given that state\n",
        "    action, _ = policy.act(state)\n",
        "    state, reward, done, info = env.step(action) # We directly put next_state = state for recording logic\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ],
      "metadata": {
        "id": "HP3vdegnaoTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo4JH45if81z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9622d8b-9094-4f70-c8e3-016afb911829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        }
      ],
      "source": [
        "# Record and save a video\n",
        "video_path =  \"replay1.mp4\"\n",
        "video_fps=60\n",
        "record_video(env, myPolicy, video_path, video_fps)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}